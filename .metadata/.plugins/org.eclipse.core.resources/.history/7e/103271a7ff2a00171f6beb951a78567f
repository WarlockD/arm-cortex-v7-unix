#ifndef _ATOMIC_H_
#define _ATOMIC_H_

//#include "conf.h"
#include <cstdint>
#include <cstddef>
#include <functional>
#include <type_traits>


namespace ARM {
	namespace PRIV {
	template<typename T, size_t size> struct EXCLUSIVE {};
	template<typename T> struct EXCLUSIVE<T,1> {
		static constexpr bool type_is_signed = std::is_unsigned<T>::value;
		static constexpr bool type_size = 1;
		using type = T;

		__attribute__((always_inline)) static inline uint32_t STREX(T value, volatile T *addr){
			uint32_t result;
			   __asm volatile ("strexb %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" (value) );
			   return(result);
		}
		__attribute__((always_inline)) static inline T LDREX(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrexb %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline T LDRT(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrbt %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void CLREX(void)
		{
		  __asm volatile ("clrex" ::: "memory");
		}
		__attribute__((always_inline)) static inline T LDRT(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrt %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void STRT(T value, volatile T *addr){
			   __asm volatile ("strt %1, %0" : "=Q" (*addr) : "r" (static_cast<uint32_t>(value)) );
		}
		__attribute__((always_inline)) static inline T XCHG(T nvalue, volatile T *ptr) {
			uint32_t tmp;
			uint32_t ret;
			__builtin_prefetch((const void*)ptr,1);
			asm volatile(
			"1:	ldrexb	%0, [%3]\n"
			"	strexb	%1, %2, [%3]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
				: "=&r" (ret), "=&r" (tmp)
				: "r" (nvalue), "r" (ptr)
				: "memory", "cc");
			return static_cast<T>(ret);
		}
		__attribute__((always_inline)) static inline T ADD(T value, T *ptr) {
				T tmp;
				int result;
				__asm__ __volatile__(
						"1:	ldrexb	%0, [%2]\n"
						"	add	%0, %0, %3\n"
						"	strexb	%1, %0, [%2]\n"
	#ifdef DISABLE_THUMB2
				"	teq	%1, #0\n"
				"	bne	1b"
	#else
				"	cbz	%1, 1b\n"
	#endif
				: "=&r" (result), "=&r" (tmp)
				: "r" (ptr), "Ir" (value)
				: "cc");
				return result;
			}
			__attribute__((always_inline)) static inline T SUB(T value, T *ptr) {
				T tmp;
				int result;
				__asm__ __volatile__(
						"1:	ldrexb	%0, [%2]\n"
						"	sub	%0, %0, %3\n"
						"	strexb	%1, %0, [%2]\n"
	#ifdef DISABLE_THUMB2
				"	teq	%1, #0\n"
				"	bne	1b"
	#else
				"	cbz	%1, 1b\n"
	#endif
				: "=&r" (result), "=&r" (tmp)
				: "r" (ptr), "Ir" (value)
				: "cc");
				return result;
			}

	};

	template<typename T> struct EXCLUSIVE<T,2> {
		static constexpr bool type_is_signed = std::is_unsigned<T>::value;
		static constexpr bool type_size = 2;
		using type = T;

		__attribute__((always_inline)) static inline uint32_t STREX(T value, volatile T *addr){
			uint32_t result;
			   __asm volatile ("strexh %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" (value) );
			   return(result);
		}
		__attribute__((always_inline)) static inline T LDREX(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrexh %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void CLREX(void)
		{
		  __asm volatile ("clrex" ::: "memory");
		}
		__attribute__((always_inline)) static inline T LDRT(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrht %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void STRT(T value, volatile T *addr){
			   __asm volatile ("strht %1, %0" : "=Q" (*addr) : "r" (static_cast<uint32_t>(value)) );
		}
		__attribute__((always_inline)) static inline T XCHG(T nvalue, volatile T *ptr) {
			uint32_t tmp;
			uint32_t ret;
			__builtin_prefetch((const void*)ptr,1);
			asm volatile(
			"1:	ldrexh	%0, [%3]\n"
			"	strexh	%1, %2, [%3]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
				: "=&r" (ret), "=&r" (tmp)
				: "r" (nvalue), "r" (ptr)
				: "memory", "cc");
			return static_cast<T>(ret);
		}
		__attribute__((always_inline)) static inline T ADD(T value, T *ptr) {
			T tmp;
			int result;
			__asm__ __volatile__(
					"1:	ldrexh	%0, [%2]\n"
					"	add	%0, %0, %3\n"
					"	strexh	%1, %0, [%2]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
			: "=&r" (result), "=&r" (tmp)
			: "r" (ptr), "Ir" (value)
			: "cc");
			return result;
		}
		__attribute__((always_inline)) static inline T SUB(T value, T *ptr) {
			T tmp;
			int result;
			__asm__ __volatile__(
					"1:	ldrexh	%0, [%2]\n"
					"	sub	%0, %0, %3\n"
					"	strexh	%1, %0, [%2]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
			: "=&r" (result), "=&r" (tmp)
			: "r" (ptr), "Ir" (value)
			: "cc");
			return result;
		}
	};
	template<typename T> struct EXCLUSIVE<T,4> {
		static constexpr bool type_is_signed = std::is_unsigned<T>::value;
		static constexpr bool type_size = 2;
		using type = T;

		__attribute__((always_inline)) static inline uint32_t STREX(T value, volatile T *addr){
			uint32_t result;
			   __asm volatile ("strex %0, %2, %1" : "=&r" (result), "=Q" (*addr) : "r" (value) );
			   return(result);
		}
		__attribute__((always_inline)) static inline T LDREX(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrex %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void CLREX(void)
		{
		  __asm volatile ("clrex" ::: "memory");
		}
		__attribute__((always_inline)) static inline T LDRT(volatile T *addr)
		{
		    uint32_t result;
		    __asm volatile ("ldrt %0, %1" : "=r" (result) : "Q" (*addr) );
		   return static_cast<T>(result);
		}
		__attribute__((always_inline)) static inline void STRT(T value, volatile T *addr){
			   __asm volatile ("strt %1, %0" : "=Q" (*addr) : "r" (static_cast<uint32_t>(value)) );
		}
		__attribute__((always_inline)) static inline T XCHG(T nvalue, volatile T *ptr) {
			uint32_t tmp;
			uint32_t ret;
			__builtin_prefetch((const void*)ptr,1);
			asm volatile(
			"1:	ldrex	%0, [%3]\n"
			"	strex	%1, %2, [%3]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
				: "=&r" (ret), "=&r" (tmp)
				: "r" (nvalue), "r" (ptr)
				: "memory", "cc");
			return static_cast<T>(ret);
		}
		__attribute__((always_inline)) static inline T ADD(T value, T *ptr) {
			T tmp;
			int result;
			__asm__ __volatile__(
					"1:	ldrex	%0, [%2]\n"
					"	add	%0, %0, %3\n"
					"	strex	%1, %0, [%2]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
			: "=&r" (result), "=&r" (tmp)
			: "r" (ptr), "Ir" (value)
			: "cc");
			return result;
		}
		__attribute__((always_inline)) static inline T SUB(T value, T *ptr) {
			T tmp;
			int result;
			__asm__ __volatile__(
					"1:	ldrex	%0, [%2]\n"
					"	sub	%0, %0, %3\n"
					"	strex	%1, %0, [%2]\n"
#ifdef DISABLE_THUMB2
			"	teq	%1, #0\n"
			"	bne	1b"
#else
			"	cbz	%1, 1b\n"
#endif
			: "=&r" (result), "=&r" (tmp)
			: "r" (ptr), "Ir" (value)
			: "cc");
			return result;
		}
	};
	template<typename T>
	using HELPER = EXCLUSIVE<T,sizeof(T)>;

#define CREATE_MSR_MRS_STRUCT(REGNAME) 											\
	struct REGNAME { 															\
		__attribute__( ( always_inline ) ) static inline uint32_t get() { 		\
		  	  uint32_t result;													\
		  	  __asm volatile ("MRS %0," #REGNAME : "=r" (result) );				\
		  	  return(result);													\
		}																		\
		__attribute__( ( always_inline ) ) static inline void set(uint32_t v) { \
			__asm volatile ("MSR " #REGNAME ", %0" : : "r" (v) : "memory");		\
		}																		\
	};
	CREATE_MSR_MRS_STRUCT(CONTROL);
	CREATE_MSR_MRS_STRUCT(IPSR);
	CREATE_MSR_MRS_STRUCT(APSR);
	CREATE_MSR_MRS_STRUCT(PSP);
	CREATE_MSR_MRS_STRUCT(MSP);
	CREATE_MSR_MRS_STRUCT(PRIMASK);
	CREATE_MSR_MRS_STRUCT(FAULTMASK);
	CREATE_MSR_MRS_STRUCT(BASEPRI);
	CREATE_MSR_MRS_STRUCT(BASEPRI_MAX);


	struct FPSCR {
		__attribute__( ( always_inline ) ) static inline uint32_t get() {
		  	  uint32_t result;
				__asm volatile ("");
				__asm volatile ("VMRS %0, fpscr" : "=r" (result) );
				__asm volatile ("");
		  	  return(result);
		}
		__attribute__( ( always_inline ) ) static inline void set(uint32_t v) {
			__asm volatile ("");
			__asm volatile ("VMSR fpscr, %0" : : "r" (fpscr) : "vfpcc");
			__asm volatile ("");
		}
	};
	template<typename T>
	struct REG {
		__attribute__( ( always_inline ) ) static inline uint32_t get() { return T::get(); }
		__attribute__( ( always_inline ) ) static inline void set(uint32_t v) { T::set(v); }
		__attribute__( ( always_inline ) ) inline REG& operator=(uint32_t v) const { T::set(v); return *this; }
		__attribute__( ( always_inline ) ) inline operator uint32_t() const { return T::get(); }
	};
};
__attribute__( ( always_inline ) ) static inline void irq_enable() { __asm volatile ("cpsie i" : : : "memory"); }
__attribute__( ( always_inline ) ) static inline void irq_disable() { __asm volatile ("cpsid i" : : : "memory"); }
__attribute__( ( always_inline ) ) static inline void fault_irq_enable() { __asm volatile ("cpsie f" : : : "memory"); }
__attribute__( ( always_inline ) ) static inline void fault_irq_disable() { __asm volatile ("cpsid f" : : : "memory"); }
__attribute__( ( always_inline ) ) static inline void nop() { __asm volatile ("nop"); }
__attribute__( ( always_inline ) ) static inline void wfi() { __asm volatile ("wfi"); }
__attribute__( ( always_inline ) ) static inline void wfe() { __asm volatile ("wfe"); }
__attribute__( ( always_inline ) ) static inline void sev() { __asm volatile ("sev"); }
__attribute__( ( always_inline ) ) static inline void isb() { __asm volatile ("isb 0xF":::"memory"); }
__attribute__( ( always_inline ) ) static inline void dsb() { __asm volatile ("dsb 0xF":::"memory"); }
__attribute__( ( always_inline ) ) static inline void dmb() { __asm volatile ("dmb 0xF":::"memory"); }

__attribute__( ( always_inline ) ) static inline void dsb_sev(void) { dmb(); nop(); }
__attribute__( ( always_inline ) ) static inline void barrier(void) { dmb();  }

using CONTROL = PRIV::REG<PRIV::CONTROL>;
using IPSR = PRIV::REG<PRIV::IPSR>;
using APSR = PRIV::REG<PRIV::APSR>;
using PSP = PRIV::REG<PRIV::PSP>;
using MSP = PRIV::REG<PRIV::MSP>;
using FAULTMASK = PRIV::REG<PRIV::FAULTMASK>;
using BASEPRI = PRIV::REG<PRIV::BASEPRI>;
using BASEPRI_MAX = PRIV::REG<PRIV::BASEPRI_MAX>;
using FPSCR = PRIV::REG<PRIV::FPSCR>;

struct IRQ_HARDWARE_STACK {
	uint32_t R0;
	uint32_t R1;
	uint32_t R3;
	uint32_t IP;
	uint32_t LR;
	uint32_t PC;
	uint32_t XPSR;
};


template<typename T, typename EX = PRIV::EXCLUSIVE<T,sizeof(T)>>
static inline bool atomic_test_and_set(T *ptr)
{
	return EX::XCHG(1,ptr) == 1;
}
template<typename T, typename U, typename EX = PRIV::EXCLUSIVE<T,sizeof(T)>>
static inline T atomic_xchg(volatile T *ptr, U v) {
	return EX::XCHG(static_cast<T>(v),ptr);
}
template<typename T, typename U, typename EX = PRIV::EXCLUSIVE<T,sizeof(T)>>
static inline T xchg(volatile T *ptr, U v) {
	return EX::XCHG(static_cast<T>(v),ptr);
}
template<typename T, typename U, typename EX = PRIV::EXCLUSIVE<T,sizeof(T)>>
static inline T atomic_add(volatile T *ptr, U v) {
	return EX::ADD(static_cast<T>(v),ptr);
}
template<typename T, typename U, typename EX = PRIV::EXCLUSIVE<T,sizeof(T)>>
static inline T atomic_sub(volatile T *ptr, U v) {
	return EX::SUB(static_cast<T>(v),ptr);
}
// dead simple atomic lock
class atomic_lock {
	uint32_t _lock;
public:
	atomic_lock() : _lock(0) {}
	~atomic_lock() { _lock = 0; }
	bool try_lock() { return !atomic_test_and_set(&_lock); }
	void lock() { while(atomic_test_and_set(&_lock)) ; }
	void unlock() { _lock = 0; }
};

template<typename T>
class lock_irq {
	uint8_t _save;
	uint8_t _prio;
public:
	lock_irq(uint8_t s=0) : _save(T::get()), _prio(s) { T::set(0); }
	inline constexpr bool enabled() const { return T::get() !=  _save;  }
	void disable() { T::set(_prio); }
	void restore() { T::set(_save); }
	~lock_irq() { T::set(_save); }
};
#define WFE(cond)			\
	"it " cond "\n\t"		\
	"wfe" cond ".n",		\
	"nop.w"					\
)

struct arch_rwlock_t{
	using EX = PRIV::EXCLUSIVE<int,sizeof(int)>;
	uint32_t _lock;
	void unlock() { _lock = 0; }
	void write_lock() {
		uint32_t tmp;
		do {
			tmp = EX::LDREX(&_lock);

		} while(tmp);
		__builtin_prefetch(&_lock,1);
		__asm__ __volatile__(
			"1:	ldrex	%0, [%1]\n"
			"	teq	%0, #0\n"
			"	it ne\n"
			"	wfene \n"
			"	strexeq	%0, %2, [%1]\n"
			"	cbz	%0, 1b\n"
		: "=&r" (tmp)
		: "r" (&rw->lock), "r" (0x80000000)
		: "cc");
		barrier();
	}
	void write_unlock() {
		barrier();
		_lock = 0;
		dsb_sev();
	}
	bool write_trylock() {
		uint32_t contended, res;
		__builtin_prefetch(&_lock,1);
		do {
			__asm__ __volatile__(
			"	ldrex	%0, [%2]\n"
			"	mov	%1, #0\n"
			"	teq	%0, #0\n"
			"	strexeq	%1, %3, [%2]"
			: "=&r" (contended), "=&r" (res)
			: "r" (&rw->lock), "r" (0x80000000)
			: "cc");
		} while (res);

		if (!contended) {
			dsb_sev();
			return true;
		} else return false;
	}
} ;
struct arch_spinlock_t{
	static constexpr uint32_t TICKET_SHIFT	=16;
	union {
		uint32_t slock;
		struct __raw_tickets {
			uint16_t owner;
			uint16_t next;
		} tickets;
	};
	void unlock() { slock = 0; }

};




static inline int arch_write_trylock(arch_rwlock_t *rw)
{


	prefetchw(&rw->lock);

}

static inline void arch_write_unlock(arch_rwlock_t *rw)
{
	smp_mb();

	__asm__ __volatile__(
	"str	%1, [%0]\n"
	:
	: "r" (&rw->lock), "r" (0)
	: "cc");

	dsb_sev();
}

/* write_can_lock - would write_trylock() succeed? */
#define arch_write_can_lock(x)		(ACCESS_ONCE((x)->lock) == 0)

/*
 * Read locks are a bit more hairy:
 *  - Exclusively load the lock value.
 *  - Increment it.
 *  - Store new lock value if positive, and we still own this location.
 *    If the value is negative, we've already failed.
 *  - If we failed to store the value, we want a negative result.
 *  - If we failed, try again.
 * Unlocking is similarly hairy.  We may have multiple read locks
 * currently active.  However, we know we won't have any write
 * locks.
 */
static inline void arch_read_lock(arch_rwlock_t *rw)
{
	unsigned long tmp, tmp2;

	prefetchw(&rw->lock);
	__asm__ __volatile__(
"1:	ldrex	%0, [%2]\n"
"	adds	%0, %0, #1\n"
"	strexpl	%1, %0, [%2]\n"
	WFE("mi")
"	rsbpls	%0, %1, #0\n"
"	bmi	1b"
	: "=&r" (tmp), "=&r" (tmp2)
	: "r" (&rw->lock)
	: "cc");

	smp_mb();
}

static inline void arch_read_unlock(arch_rwlock_t *rw)
{
	unsigned long tmp, tmp2;

	smp_mb();

	prefetchw(&rw->lock);
	__asm__ __volatile__(
"1:	ldrex	%0, [%2]\n"
"	sub	%0, %0, #1\n"
"	strex	%1, %0, [%2]\n"
"	teq	%1, #0\n"
"	bne	1b"
	: "=&r" (tmp), "=&r" (tmp2)
	: "r" (&rw->lock)
	: "cc");

	if (tmp == 0)
		dsb_sev();
}

static inline int arch_read_trylock(arch_rwlock_t *rw)
{
	unsigned long contended, res;

	prefetchw(&rw->lock);
	do {
		__asm__ __volatile__(
		"	ldrex	%0, [%2]\n"
		"	mov	%1, #0\n"
		"	adds	%0, %0, #1\n"
		"	strexpl	%1, %0, [%2]"
		: "=&r" (contended), "=&r" (res)
		: "r" (&rw->lock)
		: "cc");
	} while (res);

	/* If the lock is negative, then it is already held for write. */
	if (contended < 0x80000000) {
		smp_mb();
		return 1;
	} else {
		return 0;
	}
}

/* read_can_lock - would read_trylock() succeed? */
#define arch_read_can_lock(x)		(ACCESS_ONCE((x)->lock) < 0x80000000)

#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)
#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)

#define arch_spin_relax(lock)	cpu_relax()
#define arch_read_relax(lock)	cpu_relax()
#define arch_write_relax(lock)	cpu_relax()
using lock_irq_irq = lock_irq<PRIV::PRIMASK>;
using lock_irq_basepri = lock_irq<PRIV::BASEPRI>;

#endif
