#ifndef _ATOMIC_H_
#define _ATOMIC_H_

//#include "conf.h"
#include <cstdint>
#include <cstddef>
#include <functional>
#include <type_traits>



template<typename T, typename = typename std::enable_if<sizeof(T) == 4>::type>
static inline T test_and_set(T *word)
{
	register T result = 1;
	__asm__ __volatile__(
	    "mov r1, #1\n"
	    "mov r2, %[word]\n"
	    "ldrex r0, [r2]\n"	/* Load value [r2] */
	    "cmp r0, #0\n"	/* Checking is word set to 1 */
	    "itt eq\n"
	    "strexeq r0, r1, [r2]\n"
	    "moveq %[result], r0\n"
	    : [result] "=r"(result)
	    : [word] "r"(word)
	    : "r0", "r1", "r2");
	return result == 0;
}
template<typename T, typename = typename std::enable_if<sizeof(T) == 2>::type>
static inline T test_and_set(T *word)
{
	register T result = 1;
	__asm__ __volatile__(
	    "mov r1, #1\n"
	    "mov r2, %[word]\n"
	    "ldrexh r0, [r2]\n"	/* Load value [r2] */
	    "cmp r0, #0\n"	/* Checking is word set to 1 */
	    "itt eq\n"
	    "strexheq r0, r1, [r2]\n"
	    "moveq %[result], r0\n"
	    : [result] "=r"(result)
	    : [word] "r"(word)
	    : "r0", "r1", "r2");
	return result == 0;
}

template<typename T, typename = typename std::enable_if<sizeof(T) == 1>::type>
static inline T test_and_set(T *word)
{
	register T result = 1;
	__asm__ __volatile__(
	    "mov r1, #1\n"
	    "mov r2, %[word]\n"
	    "ldrexb r0, [r2]\n"	/* Load value [r2] */
	    "cmp r0, #0\n"	/* Checking is word set to 1 */
	    "itt eq\n"
	    "strexbeq r0, r1, [r2]\n"
	    "moveq %[result], r0\n"
	    : [result] "=r"(result)
	    : [word] "r"(word)
	    : "r0", "r1", "r2");
	return result == 0;
}


static inline  uint32_t test_and_set_bit(uint32_t *word, int bitmask)
{
	register int result = 1;

	__asm__ __volatile__(
	    "mov r2, %[word]\n"
	    "ldrex r0, [r2]\n"		/* Load value [r2] */
	    "tst r0, %[bitmask]\n"	/* Compare value with bitmask */

	    "ittt eq\n"
	    "orreq r1, r0, %[bitmask]\n"	/* Set bit: r1 = r0 | bitmask */
	    "strexeq r0, r1, [r2]\n"		/* Write value back to [r2] */
	    "moveq %[result], r0\n"
	    : [result] "=r"(result)
	    : [word] "r"(word), [bitmask] "r"(bitmask)
	    : "r0", "r1", "r2");

	return result == 0;
}

/*
 * ARMv6 UP and SMP safe atomic ops.  We use load exclusive and
 * store exclusive to ensure that these are atomic.  We may loop
 * to ensure that the update happens.
 */
// fix these with enable if to check type sizes
template<typename T, typename I, template std::enable_if<sizeof(T) == 4>::type>
inline void atomic_add(T *v, I i)
{
	T tmp;
	int result;
	__asm__ __volatile__("@ atomic_add\n"
			"1:	ldrex	%0, [%2]\n"
			"	add	%0, %0, %3\n"
			"	strex	%1, %0, [%2]\n"
			"	teq	%1, #0\n"
			"	bne	1b"
	: "=&r" (result), "=&r" (tmp)
	: "r" (v), "Ir" (i)
	: "cc");
}
template<typename I, typename T>
inline T atomic_add_return(I i, T *v)
{
	T tmp;
	T result;
	__asm__ __volatile__("@ atomic_add_return\n"
			"1:	ldrex	%0, [%2]\n"
			"	add	%0, %0, %3\n"
			"	strex	%1, %0, [%2]\n"
			"	teq	%1, #0\n"
			"	bne	1b"
	: "=&r" (result), "=&r" (tmp)
	: "r" (v), "Ir" (i)
	: "cc");
	return result;
}
template<typename I, typename T>
inline void atomic_sub(I i, T *v)
{
	T tmp;
	T result;
	__asm__ __volatile__("@ atomic_sub\n"
			"1:	ldrex	%0, [%2]\n"
			"	sub	%0, %0, %3\n"
			"	strex	%1, %0, [%2]\n"
			"	teq	%1, #0\n"
			"	bne	1b"
	: "=&r" (result), "=&r" (tmp)
	: "r" (v), "Ir" (i)
	: "cc");
}
template<typename I, typename T>
inline T atomic_sub_return(I i, T *v)
{
	T tmp;
	T result;
	__asm__ __volatile__("@ atomic_sub_return\n"
			"1:	ldrex	%0, [%2]\n"
			"	sub	%0, %0, %3\n"
			"	strex	%1, %0, [%2]\n"
			"	teq	%1, #0\n"
			"	bne	1b"
	: "=&r" (result), "=&r" (tmp)
	: "r" (v), "Ir" (i)
	: "cc");
	return result;
}
template<typename I1, typename I2,typename T>
inline T atomic_cmpxchg(T *ptr, I1 oldv,I2 newv)
{
	T oldval, res;
	do {
		__asm__ __volatile__("@ atomic_cmpxchg\n"
		"ldrex	%1, [%2]\n"
		"mov	%0, #0\n"
		"teq	%1, %3\n"
		"strexeq %0, %4, [%2]\n"
			: "=&r" (res), "=&r" (oldval)
			: "r" (ptr), "Ir" (oldv), "r" (newv)
			: "cc");
	} while (res);
	return oldval;
}
template<typename T>
inline void atomic_clear_mask(T mask, T *addr)
{
	T tmp, tmp2;

	__asm__ __volatile__("@ atomic_clear_mask\n"
"1:	ldrex	%0, [%2]\n"
"	bic	%0, %0, %3\n"
"	strex	%1, %0, [%2]\n"
"	teq	%1, #0\n"
"	bne	1b"
	: "=&r" (tmp), "=&r" (tmp2)
	: "r" (addr), "Ir" (mask)
	: "cc");
}

#endif
